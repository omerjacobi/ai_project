#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass IEEEtran
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 1cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Abalone - Introduction to artificial intelligence 
\begin_inset Newline newline
\end_inset

Final Project
\end_layout

\begin_layout Author
Rubi Mizrachi - Guy Golran - Omer Jacobi - Rom Zats 
\end_layout

\begin_layout Special Paper Notice
The Hebrew University of Jerusalem Jerusalem, Israel
\end_layout

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
we tackle the challenge of improving AI current techniques of solving Abalone
 game , using Alpha-Beta algorithm and Q-Learning based algorithm, in past
 research Alpha-Beta appeared to be superior over Q-learning agent , we
 wish in this research to achieve adequate results with our Q-learning agent
 that will outperform previous experiments, and acquire better results in
 general due to the significant growth of computer processing power.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Abalone game was invented in 1987 by Michel Lalet and Laurent Lévi,it is
 a perfect information game, which is based on the Japanese Sumo wrestling
 ,involving two-players, players are represented by opposing black and white
 marbles on a hexagonal board.
 the winner is the first player how manage to push out of the board 6 marbles
 that belongs to the other player.
\end_layout

\begin_layout Standard
for each move, a player moves a straight line of one ,two or three marbles.
 a player can push their opponent’s marbles (a “Sumito” move) , a player
 can push only if the pushing line has more marbles than the pushed line
 (three can push one or two, two can push one).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial state
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\begin_inset Graphics
	filename Screenshot from 2018-08-22 12-35-04.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Abalone - Is it suitable for A.I.
 learning? 
\end_layout

\begin_layout Standard
Abalone is a board game that involves no random choices (i.e no cubes is
 involved) , therefore, just like chess , the game tree can be fully observed
 , meanwhile , one of the reasons we choose abalone is that it tends to
 be less complex than chess.
 while in chess there is various kind of pieces (king, queen etc) which
 force an exponential number of different game states, therefore calculating
 the game tree is hard , and the complexity is really high.so although abalone
 is similar to chess in some ways, one of the main reasons we chose it is
 because of the marbles - the marbles are all of the same kind, therefore
 there is a lot of redundant of game states , in our research we try to
 exploit this fact and use it in our favor to spare important time.
 
\end_layout

\begin_layout Section
 How Complex is it ?
\end_layout

\begin_layout Standard
Games complexity can be interpreted in two ways: 
\end_layout

\begin_layout Subsection
state-space complexity:
\end_layout

\begin_layout Standard
The state-space complexity of a game is defined as the number of legal game
 positions reachable from the initial position of the game.
 All situations where one player has nine marbles and the other less than
 eight are illegal (since the game ends when a player loses the sixth marble).
 The state-space complexity can therefore be approximated by the following
 formula:
\begin_inset Formula 
\[
\sum_{k=8}^{14}\sum_{k=8}^{14}\frac{61!}{k!\left(61-k\right)!}\cdot\frac{\left(61-k\right)!}{m!\left(\left(61-k\right)-m\right)!}
\]

\end_inset

this approximation is an upper bound of the state complexity, because abalone
 board is symmetric , and all of the marbles is the same , every state can
 repeat occur in six different angles, and all state can occur twice in
 a mirror point of view (switching the players does not change the state
 because the players are symmetric), therefore after dividing by 12 we will
 get that the state complexity of approximately is 
\begin_inset Formula $6.5\cdot10^{23}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
high game-tree complexity:
\end_layout

\begin_layout Standard
in order to calculate game-tree complexity we need to calculate the number
 of leaf nodes in the complete game tree.
 The game tree is typically vastly larger than the state space in most of
 the games and most of the times calculating the size of the game tree is
 impossible , but a reasonable estimate can be made.
 in order to do so we need to know the the average branching factor of the
 tree and the number of ply (half-moves, one turn of one of the players)
 of the game.
 In abalone we consider a ‘node’ to be a legal position, the average branching
 factor is about 60.
 This means that at each move , on average , a player has 60 legal moves
 and so , for each legal position there are on average 60 positions that
 can follow (when a move is made).
 The average game-length of abalone is 87 ply, and with avg branching factor
 of 60 we get tree complexity of: 
\begin_inset Formula $5\cdot10^{154}$
\end_inset

.
\end_layout

\begin_layout Standard
This game tree complexity lies between the game-tree complexity of ‘Xiangqi’
 and ‘Shogi’ as presented below:
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Game distribution based on complexity
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\begin_inset Graphics
	filename Screenshot from 2018-08-22 12-36-19.png
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Project Goals 
\end_layout

\begin_layout Standard
In this project we wish to answer the following questions: 
\end_layout

\begin_layout Itemize
Does abalone can be played well by a computer ? can it outperform advanced
 players even using more trivial algorithms like Alpha-Beta ? 
\end_layout

\begin_layout Itemize
Which algorithm is most suitable for this game ?
\end_layout

\begin_layout Itemize
Can Q-learning show better results with this game than previous experiments?
 
\end_layout

\begin_layout Itemize
Does the growth of computing power in the last decade (as to multiprocessing
 power that involved significantly) will provide tremendous improvements
 in the results we get compared to previous researches ? 
\end_layout

\begin_layout Section
Our Approach 
\end_layout

\begin_layout Standard
This section describes the techniques and algorithms used in our research
 in order answer all of the above questions, we implemented the Alpha-Beta
 algorithm and a deep Q-learning agent and tried to compare them and improve
 them.
\end_layout

\begin_layout Subsection
Alpha-Beta tree search
\end_layout

\begin_layout Standard
The first approach tested was Alpha-Beta tree search - A search algorithm
 that seeks to decrease the number of nodes that are evaluated by the MiniMax
 algorithm in its search tree.
 It is an adversarial search algorithm used commonly for machine playing
 of two-player games (Chess, Go, etc.).
 It stops completely evaluating a move when at least one possibility has
 been found that proves the move to be worse than a previously examined
 move.
 Such moves need not be evaluated further.
 When applied to a standard MiniMax tree, it returns the same move as MiniMax
 would, but prunes away branches that cannot possibly influence the final
 decision.
 
\end_layout

\begin_layout Standard
This approach proved strong enough to win against us in most games even
 for rater modest search depth of 4 (But we are not the best players), iteration
s allowing deeper vision proved better but were practically unplayable since
 average game duration was around 5 hours (see in results), we did try to
 improve performance in few ways.
\end_layout

\begin_layout Subsubsection
Alpha-Beta Improvements 
\end_layout

\begin_layout Standard
One of the reasons we decided to implement Alpha-Beta algorithm is that
 it seems to be the most compatible with Abalone, but meanwhile, due to
 the great complexity of Abalone , it is very hard to explore a tree in
 deeper depths of it.
 therefore we tried to find some various improvements that will ease on
 our search through states and improve running time ,also one of the reasons
 we choose Alpha-Beta over MiniMax is pruning which is one possible improvement
 that might help our efficiency.
 we also decided to try these several methods of improvements: Multiprocessing
 with Alpha-Beta , Minimal window search, move ordering and transportation
 table.
\end_layout

\begin_layout Paragraph
Multiprocessing with Alpha-Beta
\end_layout

\begin_layout Standard
We wanted to use in our favor one of the biggest improvements we had in
 last several years in terms of processors which is multiprocessing, we
 started to explore which way is the best to combine our multiprocessing
 power in our program, we thought about combining our multi-cores in the
 actual alpha-beta search, but it turned out to be very difficult and complex
 to do alpha beta search in parallel , there are a lot of dependencies between
 all of the processors making it very hard to co-work all together.
 We examined the results of our program and noticed that one of the most
 consuming things we done is state generation - in order to generate a proper
 game state in abalone we had to supply all N possible actions from state
 X , that will generate N distinct possible states, so when thinking it
 through , the best way to achieve all of the legal actions is to check
 every combination of 3,2,1 marbles, and if they are legal group of marbles
 (stands next to each other in a straight line position), so if we try to
 approximate the running time it takes - if our game state is mid-game position,
 let say we still have 14 marbles in the possession of our alpha-beta agent,
 so we need to check 
\begin_inset Formula $(14\cdot13\cdot12+14\cdot13+14)$
\end_inset

 options, and every single one of those options of group needs to be checked
 in several methods that require significant processing time, therefore
 we decided to try and split all of those options of possible groups among
 all processors we had (4).
 we sent each processor a chunk of combinations of marbles and got back
 in return only the valid groups they found (each processor run the desired
 methods in order to check if the current group is valid).
 Unfortunately the results were bad , and didn't came close to improve our
 running time by 4X (we used four cores rather than one).
 one of the complications about that were using Python for our project,
 which is very inefficient with running multiple threads and process.
 
\end_layout

\begin_layout Paragraph
Move ordering
\end_layout

\begin_layout Standard
We concluded from our knowledge of Alpha-Beta algorithm that it may spare
 a lot of time , but it can also be ineffective - it depends of the order
 of the nodes that are being examined in the algorithm, Therefore, the search
 algorithm should consider the most promising moves first to maximize the
 chance of tree branches being pruned ,In Abalone the strongest moves are
 those that attack the opponent or those that keep marbles together in a
 group for a better defense.
 Therefore, moves consisting of several marbles are often more valuable.
 So we investigated first moves consisting of 3 marbles.
 Then the moves containing two marbles follow and moves containing only
 a single marble are investigated last.
 The calculation time decreases dramatically as will be shown in experiments
 section below.
\end_layout

\begin_layout Paragraph
Transportation table
\end_layout

\begin_layout Standard
We noticed that Abalone is a game which is very repetitive in terms of game
 tree and states representation, as we reminded above, one of difference
 of Abalone from Chess is the fact that the marbles are all of the same,
 therefore, each state can be caused from several different series of actions
 , and this is where transportation table comes handy,our transportation
 table is hash table, which maps between a string representation of a state
 to it’s score, so if we came across a particular state before, we could
 check it's score in O(1) , instead of rechecking this specific state (and
 possibly it's child nodes) all over again, we spare that time using the
 transportation table,as will be presented below in the experiments section.
\end_layout

\begin_layout Subsubsection
Evaluation function 
\end_layout

\begin_layout Standard
In addition to focusing on improving Alpha-Beta in general we focused at
 finding good heuristics for our Abalone game solver, we know that a good
 heuristic means efficient exploration of the game tree, meaning improvement
 of running time and performance.
\begin_inset Newline newline
\end_inset

The use of heuristics is useless if we search the whole tree - if we go
 thorough all of the tree, it means we reached all of the final states in
 our game, meaning we know , what actions can lead us to a winning leaf
 or a losing leaf.
 Heuristic comes in handy once we have high branching factor (i.e Abalone)
 and we can't explore all of the game tree and we can only reach to certain
 depth and we want to estimate this state, define it as good or bad, or
 even give it a numeric score, for doing that we need to find some features
 representing the value of specific state.
 one of the demands of a good evaluation function is to be fast and accurate
 in assessing a given game position.
 in our research we focused on some specific features (and their combinations)
 in order to find the best evaluation function there is.
\end_layout

\begin_layout Paragraph
Defensive positions
\end_layout

\begin_layout Standard
While playing the game we noticed that if our marbles are spread in the
 board it is more probable that they will be attacked by the opponent, therefore
, we assumed that if our marbles will be grouped together they are less
 probable to be pushed (attacked) and if they will be in the center of the
 board we will also be less exposed to situations of losing our marbles
 (in order to loss marble it got to be close to the borders of the board).
 also by being in a 
\begin_inset Quotes eld
\end_inset

Defensive position
\begin_inset Quotes erd
\end_inset

 we can prevent the opponent being there himself, the opponent will have
 hard time grouping together if we will control the center of the board.
\end_layout

\begin_layout Paragraph
Attacking and attacked (Sumito positions)
\end_layout

\begin_layout Standard
A Sumito move is a move where some opponent’s marbles are pushed by moving
 some of the player’s own marbles.
 A Sumito situation is reached in case that there is a row of marbles where
 on one side there are only black marbles and on 8 the other side there
 are white marbles.
 Between the marbles there must not be a gap.
 If the number of marbles of one player exceeds the number of the other
 player’s marbles he is in a Sumito situation.
 We tried to value a state according to the difference between the the Sumito
 situation we got and the Sumito situation our opponent have.
 We seek to reach to situations in which we have as much as Sumito situations
 as we can and meanwhile as less as we can to our opponent , the rational
 behind that is that we will be able to control the location of the opponent
 marbles (and maybe even control the number of marbles he got) and still
 minimize the number of times he will be able to affect our moves (if he
 has low Sumito situations it means he can't push as, and of course - winning
 us).
\end_layout

\begin_layout Paragraph
Lost marbles 
\end_layout

\begin_layout Standard
We know that according to the game rules - the first player that manages
 to push 6 marbles of the other player is the winner of the game , therefore
 we need to evaluate a state by the difference between the opponent lost
 marbles and our lost marbles.
 if we pursue states in which we got more marbles than the opponent , we
 probably rise or probability of becoming the winners.
 Furthermore, ejecting an opposing marble weakens the position of the opponent
 while it strengthens the own position, because the more marbles one has
 the better one can attack and defend.
\end_layout

\begin_layout Paragraph
Win or loss
\end_layout

\begin_layout Standard
We want to give a excessively good score to states that represent victory
 to us (states in which the opponent has only 8 marbles while Monte-Carlo
 Search we got more than 8) therefore we also calculated and scored states
 according to their winning
\backslash
losing 
\begin_inset Quotes eld
\end_inset

feature
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Paragraph
Combinations
\end_layout

\begin_layout Standard
To improve the quality of the evaluation function different configurations
 of the above heuristics and different weightings were defined.
 Players using these different configurations played several games against
 a random move agent.
 The two best combinations where tested by playing against each other.
 By the outcome of those games it was assessed which player performs best.
 It turned out that both agents used the same 
\begin_inset Quotes eld
\end_inset

sub-heuristics
\begin_inset Quotes erd
\end_inset

 with different weights.
\end_layout

\begin_layout Standard
(*) Defensive heuristic: Careful trail and error process was used in order
 to achieve a heuristic that is focused on allowing the opponent as few
 attacking opportunities while still favoring opponent marble taking (if
 it is beneficial)
\end_layout

\begin_layout Standard
(*) Aggressive heuristic: Same careful trail and error process was used
 in order to achieve a heuristic that is more focused on taking opponent
 marbles but still trying to allow as few attacking opportunities to the
 opponent
\end_layout

\begin_layout Subsection
Q-learning
\end_layout

\begin_layout Standard
The second approach tested was Q-learning - A reinforcement learning technique.
 The goal of Q-Learning is to learn a policy, which tells an agent what
 action to take under what circumstances.
 
\end_layout

\begin_layout Standard
We tried to model the problem such that positive reward will be given for
 enemy marble lose and for winning, negative reward was given for lost marbles
 and lost games.
\end_layout

\begin_layout Standard
Few different approaches were used 
\end_layout

\begin_layout Subsubsection
Naive Q-learning 
\end_layout

\begin_layout Standard
In this case we ran the learning process simply with a game state and a
 possible move as input and tried to give each given state a reward.
 due to the very large game-state space that the problem generates.
 The learning process was very slow - too slow for long enough learning
 period which made the resulting agent very bad performing.
 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Win rate of Naive Q-learner against different agents 
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\begin_inset Graphics
	filename Screenshot from 2018-08-22 12-36-19.png
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Q-learning approximation
\end_layout

\begin_layout Standard
To overcome the above problem we tried to combine our Q-learning agent with
 function approximation.
 This method makes it possible to apply the algorithm to larger problems,
 even when the state space is continuous.
 It does that using an approximation function 
\end_layout

\begin_layout Standard
It turned out that this tactic was again not good enough (the learning process
 was fast but didn't produce good results).
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Win rate of approximation Q-learner against different agents 
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\begin_inset Graphics
	filename Screenshot from 2018-08-22 12-36-19.png
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Q-learning approximation with artificial neural network
\end_layout

\begin_layout Standard
Since Q-learning approximation is best used for models with immediate reward
 which our problem is lacking (since marble lose/win or game win occur very
 rarely and 
\begin_inset Quotes eld
\end_inset

bad
\begin_inset Quotes erd
\end_inset

 moves can be counted as moves that lead to marble lose in the future).
 We decided to incorporate a neural-network architecture to the Q-learning
 approximation process.
 We used the same 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
blah
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
blah
\end_layout

\end_body
\end_document
